{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAIRNESS IN MACHINE LEARNING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness is a confusing concept. Fairness is commonly defined as the quality or state of\n",
    "being fair, especially fair or impartial treatment. But what’s fair can mean different things\n",
    "in different contexts to different people.\n",
    "1. Fairness has different definitions across disciplines too. This is captured in a paper by\n",
    "Deirdre Mulligan, Joshua Kroll, Nitin Kohli and Richmond Wong.\n",
    "2. Law: fairness includes protecting individuals and groups from discrimination or mistreatment with a focus on prohibiting behaviors, biases and basing decisions on certain protected factors or social group categories.\n",
    "Social science: “often considers fairness in light of social relationships, power dynamics, institutions and markets.”\n",
    "3. Members of certain groups (or identities) that tend to experience advantages.\n",
    "Quantitative fields (i.e. math, computer science, statistics, economics): questions of\n",
    "fairness are seen as mathematical problems. Fairness tends to match to some sort\n",
    "of criteria, such as equal or equitable allocation, representation, or error rates, for a\n",
    "particular task or problem.\n",
    "Philosophy: ideas of fairness “rest on a sense that what is fair is also what is morally\n",
    "right.”\n",
    "4. Political philosophy connects fairness to notions of justice and equity.\n",
    "Even within disciplines, definitions can differ. It’s no wonder then that fairness in machine\n",
    "learning systems has caused confusion. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML researchers and practitioners tend to use a quantitative perspective as the primary\n",
    "lens for fairness. They focus on constructing an optimal ML model subject to fairness\n",
    "constraints (a “constrained optimization problem”). The constraints the model is subjected to can be informed from law, social science and philosophy perspectives.\n",
    "Commonly, constraints tend to be around sensitive, legally protected attributes. ML researchers and practitioners want the model to perform as optimally as possible while\n",
    "also treating people “fairly” with respect to these sensitive attributes. Fairness can be\n",
    "defined at the individual level (such as ensuring that similar individuals are treated similarly) or at the group level. In the latter case, this is done by grouping people into categories and ensuring that the groups are treated somewhat equitably. While fairness for a\n",
    "group can be formulated in different ways, the simplest is pursuing demographic parity\n",
    "across different subgroups (meaning each subgroup receives the positive outcome at\n",
    "equal rates / the same proportion). With demographic parity, membership in a protected\n",
    "class should have no correlation with the decision.\n",
    "This quantitative approach can be problematic. Approaches tend to be narrowly specified and don’t always capture the nuances and various conceptions of fairness. Pursuing\n",
    "demographic parity in particular may seem like a good solution but is a simplistic approach to fairness that can still be at odds with other definitions of fairness5\n",
    " -- such as\n",
    "justice. Also, even if satisfying demographic parity based on gender, for example, when\n",
    "overlaying race on top of gender, this parity can be off. It’s also important to not only look\n",
    "at parity in terms of how ML systems allocate resources, but also in how they choose not\n",
    "to allocate resources. See Box 1 for an example to make this more clear. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
